{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from transformers import pipeline\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "import string\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from googletrans import Translator\n",
    "engine = create_engine('postgresql://postgres:*Your_Database_Password*@localhost:5432/DIY')\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate with your Hugging Face token\n",
    "# login(token=\"Your Hugging Face token\")# If needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_competitor_name = \"Grand Lucky\"\n",
    "conf_sql_name = \"Review_Maps_\" + conf_competitor_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql(conf_sql_name,con=engine)\n",
    "df = data[data['date'].str.contains('tahun')==False]\n",
    "df['rating'] = df['rating'].replace(to_replace=['bintang','star'],value=\"\",regex=True)\n",
    "df['rating'] = df['rating'].astype(int)\n",
    "print(\"Total Data : \",df.shape,\"\\n\",data.rating.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['store'].nunique())\n",
    "df.head()\n",
    "df = df.head(20) # Just sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory_stemmer = StemmerFactory()\n",
    "stemmer = factory_stemmer.create_stemmer()\n",
    "\n",
    "stop_words_extend = ['abis','tempat', 'ada', 'adalah', 'agak', 'aja', 'amat', 'andaikata', 'aneh', 'anw', 'apa','klo', \n",
    "                     'apalagi', 'apapun', 'atas', 'atau', 'awalnya', 'ayo', 'bagai', 'bagaimana', 'bagaimanapun', \n",
    "                     'bahkan', 'bakal', 'balik', 'banyak', 'baru', 'bawah', 'beberapa', 'begitu', 'belum', 'biasa', \n",
    "                     'biasanya', 'bikin', 'bisakah', 'boleh', 'bro', 'bsok', 'btw', 'bu', 'buat', 'bukan', 'bukannya', \n",
    "                     'cmn', 'coba', 'cukup', 'dah', 'dahulu', 'dalam', 'dan', 'dari', 'deh', 'dekat', 'dengan', 'di', \n",
    "                     'dia', 'dikit', 'dimana', 'diri', 'dirinya', 'dl', 'dlm', 'doang', 'dong', 'dr', 'dua', 'eh', \n",
    "                     'engga', 'ga', 'gak', 'gimana', 'gini', 'gitu', 'gmn', 'gt', 'gua', 'haha', 'hai', 'halo', 'hampir', \n",
    "                     'hanya', 'hari', 'hehe', 'hihi', 'hingga', 'ini', 'itu', 'iya', 'jadi', 'jadinya', 'jalan', 'jangan', \n",
    "                     'jauh', 'jd', 'jelas', 'jg', 'juga', 'kalau', 'kalian', 'kalo', 'kami', 'kan', 'kapan', 'karena', 'kau', \n",
    "                     'kayak', 'kayaknya', 'ke', 'kecil', 'keluar', 'kemana', 'kembali', 'kemudian', 'kenapa', 'kini', 'kita', \n",
    "                     'kl', 'kmrn', 'kok', 'kondisi', 'krn', 'kurang', 'lagi', 'lah', 'lain', 'lalu', 'lbh', 'lmao', 'loh', 'lol', \n",
    "                     'makanya', 'makasih', 'mana', 'mas', 'masih', 'mbak', 'melalui', 'memang', 'mereka', 'meski', 'mesti', 'minta', \n",
    "                     'misalnya', 'moga', 'mohon', 'msh', 'mudah', 'mulai', 'nah', 'namun', 'nanti', 'ngapain', 'ngerti', 'nggak', \n",
    "                     'nih', 'nya', 'nyata', 'oke', 'oleh', 'pada', 'padanya', 'pak', 'paling', 'pastinya', 'per', 'pernah', 'plg', \n",
    "                     'plis', 'pls', 'pula', 'pun', 'rupanya', 'saat', 'saja', 'saling', 'sama', 'sampai', 'sampe', 'sangat', 'satu', \n",
    "                     'saya', 'sdh', 'sebab', 'sebagian', 'sebaiknya', 'sebelum', 'sebelumnya', 'seberapa', 'sebuah', 'sedangkan', \n",
    "                     'sedikit', 'segera', 'seharusnya', 'sehingga', 'sekali', 'selain', 'selalu', 'selama', 'seluruh', 'semacam', \n",
    "                     'semakin', 'sementara', 'semua', 'sendiri', 'seperti', 'sering', 'serta', 'sesuai', 'setelah', 'setiap', 'siap', \n",
    "                     'siapa', 'sini', 'sis', 'situ', 'skrg', 'sm', 'smg', 'soalnya', 'sudah', 'supaya', 'sy', 'tadi', 'tanpa', 'tau', \n",
    "                     'tdk', 'tentang', 'tentu', 'terkadang', 'terlalu', 'termasuk', 'terus', 'tetap', 'thx', 'tidak', 'toh', 'tp', 'tq', \n",
    "                     'trs', 'tuh', 'udah', 'umumnya', 'untuk', 'walau', 'walaupun', 'wkwk', 'ya', 'yaitu', 'yakin', 'yang', 'yuk']\n",
    "\n",
    "factory_stopword = StopWordRemoverFactory()\n",
    "stop_words = factory_stopword.get_stop_words()\n",
    "stop_words.extend(stop_words_extend)\n",
    "factory_stopword.stop_words = stop_words\n",
    "stopword_remover = factory_stopword.create_stop_word_remover()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = 0\n",
    "# fungsi untuk langkah case folding\n",
    "def casefolding(text):\n",
    "  text = text.lower()                               # Mengubah teks menjadi lower case\n",
    "  text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # Menghapus URL\n",
    "  text = re.sub(r'[-+]?[0-9]+', '', text)           # Menghapus angka\n",
    "  text = re.sub(r'[^\\w\\s]','', text)                # Menghapus karakter tanda baca\n",
    "  text = text.strip()\n",
    "  return text\n",
    "\n",
    "def text_normalize(text):\n",
    "  key_norm = pd.read_csv('https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/key_norm.csv')\n",
    "  text = ' '.join([key_norm[key_norm['singkat'] == word]['hasil'].values[0] if (key_norm['singkat'] == word).any() else word for word in text.split()])\n",
    "  text = str.lower(text)\n",
    "  return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "  text = stopword_remover.remove(text)\n",
    "  return text\n",
    "\n",
    "def stemming(text):\n",
    "  text = stemmer.stem(text)\n",
    "  return text\n",
    "\n",
    "def text_preprocessing_process(text):\n",
    "  global inc \n",
    "  inc += 1\n",
    "  try :\n",
    "    print(inc)\n",
    "    text = casefolding(text)\n",
    "    text = text_normalize(text)\n",
    "    text = remove_stop_words(text)\n",
    "    text = stemming(text)\n",
    "    clear_output()\n",
    "    return text\n",
    "  except:\n",
    "    return \"\"\n",
    "df['clean_text']  = df['review'].apply(lambda text : text_preprocessing_process(text))\n",
    "#Done 14m 48s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order_col = ['competitor','store','Province','Region','review','clean_text','reviewer_name','date','rating']\n",
    "df = df[new_order_col]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sentiment Generate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pretrained_name = \"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
    "\n",
    "predictor = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=pretrained_name,\n",
    "    tokenizer=pretrained_name\n",
    ")\n",
    "counter =0 \n",
    "def sentiment_results(text_review):\n",
    "    try :\n",
    "        global counter\n",
    "        counter += 1\n",
    "        print(counter)\n",
    "        text = text_review.replace(\"\\n\", \" \")\n",
    "        results = predictor(text_review)\n",
    "        score = results[0]['score']\n",
    "        if score <= 0.89:\n",
    "            labels = 'neutral'\n",
    "        else : \n",
    "            labels = results[0]['label']\n",
    "        clear_output()\n",
    "        return labels,score\n",
    "    except:\n",
    "        return \"\",0\n",
    "df[['sentiment','Conf']] = df['review'].apply(lambda x : pd.Series(sentiment_results(x)))\n",
    "print(\"Done generated sentiment\",df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Zero-Shot Classification**\n",
    "**Model References**\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MoritzLaurer/zeroshot-classifier/main/v2_synthetic_data/results/zeroshot-v2.0-aggreg.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "https://huggingface.co/facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_english = pipeline(\"zero-shot-classification\", model='MoritzLaurer/deberta-v3-large-zeroshot-v2.0',device=0)#Model hanya 1 bahasa (English)\n",
    "classifier_indo = pipeline(\"zero-shot-classification\", model='MoritzLaurer/bge-m3-zeroshot-v2.0',device_map=\"auto\")#Multi language Model (Include Indonesia)\n",
    "list_labels = [\"Good Sentiment\", \"Bad Staff Services\", \"Payment Issue\", \"Bad Store Environment\", \"Price Issue\", \"Bad Product\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_labels_try = [\"Good Sentiment\",\n",
    "                   \"Good Staff Services\", \"Bad Staff Services\", \n",
    "                   \"Bad Payment Procedure\", \"Good Payment Procedure\", \n",
    "                   \"Bad Store Environment\", \"Good Store Environment\", \n",
    "                   \"Low Price\", \"High Price\", \n",
    "                   \"Bad Product Quality\",\"Good Product Quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "def generate_text_classification(classifier_indo,classifier_english,list_labels,review_text,review_rating):\n",
    "    global counter\n",
    "    counter +=1\n",
    "    # clear_output()\n",
    "    # print(counter)\n",
    "    try :\n",
    "        review_translation = Translator().translate(review_text, src=\"id\", dest=\"en\").text\n",
    "        result_en = classifier_english(review_translation, candidate_labels=list_labels)\n",
    "        top_label_en = result_en['labels'][0]\n",
    "        top_conf_en = result_en['scores'][0]\n",
    "\n",
    "        result_id = classifier_indo(review_text, candidate_labels=list_labels)\n",
    "        top_label_id = result_id['labels'][0]\n",
    "        top_conf_id = result_id['scores'][0]\n",
    "\n",
    "        sentiment_id = classifier_indo(review_text, candidate_labels=[\"Positive\",\"Negative\",\"Neutral\"])\n",
    "        top_sentiment_id = sentiment_id['labels'][0]\n",
    "        top_conf_sentiment_id = sentiment_id['scores'][0]\n",
    "\n",
    "        sentiment_en = classifier_english(review_translation, candidate_labels=[\"Positive\",\"Negative\",\"Neutral\"])\n",
    "        top_sentiment_en = sentiment_en['labels'][0]\n",
    "        top_conf_sentiment_en = sentiment_en['scores'][0]\n",
    "\n",
    "        print(review_text)\n",
    "        print(\"en \",top_label_en,\":\",top_conf_en)\n",
    "        print(\"en \",top_sentiment_en,\":\",top_conf_sentiment_en)\n",
    "        print(\"id \",top_label_id,\":\",top_conf_id)\n",
    "        print(\"id \",top_sentiment_id,\":\",top_conf_sentiment_id)\n",
    "\n",
    "        if (top_conf_en > top_conf_id) and (top_conf_en>=0.7):\n",
    "            return top_label_en\n",
    "        elif (top_conf_en < top_conf_id) and (top_conf_id>=0.7):\n",
    "            return top_label_id\n",
    "        elif (top_conf_en < 0.7) or (top_conf_id<0.7):\n",
    "            print(\"review_rating \",review_rating)\n",
    "            if (review_rating <=3) and (top_label_en!=\"Good Sentiment\") and (top_label_id==top_label_en)  :\n",
    "                return top_label_en\n",
    "            elif (review_rating >=4) and (top_label_en==\"Good Sentiment\") and (top_label_id==top_label_en)  :\n",
    "                return top_label_en\n",
    "            elif (top_label_id != top_label_en) and (top_conf_en < top_conf_id):\n",
    "                return f\"Check_{top_label_id}\"\n",
    "            elif (top_label_id != top_label_en) and (top_conf_en > top_conf_id):\n",
    "                return f\"Check_{top_label_en}\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    except :\n",
    "        return \"\"\n",
    "\n",
    "df['Labels'] = df.apply(lambda data: generate_text_classification(classifier_indo,classifier_english,list_labels_try,data['review'],data['rating']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Menambahkan tanggal dan waktu ke nama file\n",
    "filename_version = f\"Sentiment_{conf_competitor_name}_{current_time}.csv\"\n",
    "df.to_csv(filename_version, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlowGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
